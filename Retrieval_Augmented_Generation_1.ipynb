{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Msaleemakhtar/GenerativeAI/blob/main/Retrieval_Augmented_Generation_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **🌟 Retrieval-Augmented Generation (RAG) 🌟 with 🛠️ Langchain 🚀**"
      ],
      "metadata": {
        "id": "jTLvtd6hP5BD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain_community tiktoken  langchainhub langchain langchain-google-genai"
      ],
      "metadata": {
        "id": "zsmvbXaQQGbB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "b6fee8d9-d5d9-4ec7-9fb6-0d2809c5ab70"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting langchainhub\n",
            "  Downloading langchainhub-0.1.21-py3-none-any.whl.metadata (659 bytes)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.10.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.0 (from langchain_community)\n",
            "  Downloading langchain_core-0.3.5-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.112 (from langchain_community)\n",
            "  Downloading langsmith-0.1.126-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.5.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain_community)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchainhub) (24.1)\n",
            "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
            "  Downloading types_requests-2.32.0.20240914-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: google-generativeai<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from langchain-google-genai) (0.7.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.11.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.6 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.6.6)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (2.137.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (3.20.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (1.24.0)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.0->langchain_community)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.112->langchain_community)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.112->langchain_community)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (1.65.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (4.9)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain_community) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain_community)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain_community) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain_community)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain_community)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (1.48.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (3.1.4)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain_community) (1.2.2)\n",
            "Downloading langchain_community-0.3.0-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchainhub-0.1.21-py3-none-any.whl (5.2 kB)\n",
            "Downloading langchain-0.3.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_google_genai-2.0.0-py3-none-any.whl (39 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_core-0.3.5-py3-none-any.whl (399 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.9/399.9 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.126-py3-none-any.whl (290 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.5.2-py3-none-any.whl (26 kB)\n",
            "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading types_requests-2.32.0.20240914-py3-none-any.whl (15 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: types-requests, tenacity, python-dotenv, orjson, mypy-extensions, marshmallow, jsonpointer, h11, typing-inspect, tiktoken, langchainhub, jsonpatch, httpcore, pydantic-settings, httpx, dataclasses-json, langsmith, langchain-core, langchain-text-splitters, langchain-google-genai, langchain, langchain_community\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed dataclasses-json-0.6.7 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.0 langchain-core-0.3.5 langchain-google-genai-2.0.0 langchain-text-splitters-0.3.0 langchain_community-0.3.0 langchainhub-0.1.21 langsmith-0.1.126 marshmallow-3.22.0 mypy-extensions-1.0.0 orjson-3.10.7 pydantic-settings-2.5.2 python-dotenv-1.0.1 tenacity-8.5.0 tiktoken-0.7.0 types-requests-2.32.0.20240914 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QD13N1NVTpGS",
        "outputId": "a9b56e57-b138-4999-9386-e52584da0a66"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.2/599.2 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.1/164.1 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for durationpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "# os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "# os.environ['LANGCHAIN_API_KEY'] = \"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\"\n",
        "# os.environ[\"LANGCHAIN_PROJECT\"]=\"rag-prompt\"\n",
        "\n",
        "\n",
        "\n",
        "LANGCHAIN_TRACING_V2 = 'true'\n",
        "LANGCHAIN_ENDPOINT = 'https://api.smith.langchain.com'\n",
        "LANGCHAIN_API_KEY= \"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\"\n",
        "LANGCHAIN_PROJECT=\"rag-prompt\"\n"
      ],
      "metadata": {
        "id": "15bUJhvE2Qzk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "#GOOGLE_API_KEY=\"AIzaSyCaDox1OPxkMI6TUjeE9TSkDAGdOR7wqEE\"\n",
        "\n",
        "\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = getpass.getpass('Gemini API Key:')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mm1tvIgbXKYr",
        "outputId": "ede89041-8a9f-46a2-f01c-c784b72e9e84"
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gemini API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "\n",
        "\n",
        "\n",
        "#### INDEXING ####\n",
        "\n",
        "# Load Documents\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# Embed\n",
        "gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "                     documents=splits,                 # Data\n",
        "                     embedding=gemini_embeddings,    # Embedding model\n",
        "                     persist_directory=\"./chroma_db\" # Directory to save data\n",
        "                     )\n",
        "\n",
        "\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "#### RETRIEVAL and GENERATION ####\n",
        "\n",
        "# Prompt\n",
        "#The prompt template 'hwchase17/rag-prompt' does not exist. Using a different template instead.\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "# LLM\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",\n",
        "                 temperature=0.7, top_p=0.85)\n",
        "\n",
        "# Post-processing\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# Chain\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Question\n",
        "rag_chain.invoke(\"What is Task Decomposition?\")"
      ],
      "metadata": {
        "id": "lh5S8MtmQGKm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "3121e026-0cf1-4f4d-8cac-d046956fb44e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
            "/usr/local/lib/python3.10/dist-packages/langsmith/client.py:323: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Task decomposition is the process of breaking down a complex task into smaller, more manageable steps. This can be achieved by prompting a language model (LLM) to \"think step by step,\" using task-specific instructions, or with human input. The goal is to make the task easier to understand and execute. \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part-2 Indexing**"
      ],
      "metadata": {
        "id": "3Q5RLpbUKEN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0FTZDe3KWnp",
        "outputId": "abf0eb81-66da-47cc-d916-386a349c41fa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Documents\n",
        "question = \"What kinds of pets do I like?\"\n",
        "document = \"My favorite pet is a cat.\""
      ],
      "metadata": {
        "id": "XzMR6QRbQGGg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    encoding = tiktoken.get_encoding(encoding_name)\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens\n",
        "\n",
        "num_tokens_from_string(question, \"cl100k_base\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgkkc1JBtqOz",
        "outputId": "6a05f9f6-9a40-4d8c-f30e-b11155a53a2f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "embd = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "query_result = embd.embed_query(question)\n",
        "document_result = embd.embed_query(document)\n",
        "len(query_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2TRi9ZItqJe",
        "outputId": "d4553e6c-c0d6-4203-c5ad-420a94e0eaa1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "    return dot_product / (norm_vec1 * norm_vec2)\n",
        "\n",
        "similarity = cosine_similarity(query_result, document_result)\n",
        "print(\"Cosine Similarity:\", similarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fg7lkJ9EtqCC",
        "outputId": "41fde64e-9409-422f-e70b-c8b0cedc8f0a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity: 0.853565216692013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#### INDEXING ####\n",
        "\n",
        "# Load Documents\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "Or-Hg5cutp_G"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)\n"
      ],
      "metadata": {
        "id": "s6zdTvCAtp8P"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splits[1]"
      ],
      "metadata": {
        "id": "yINgg5VCtp57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee683b32-07c1-491a-9aca-fc0146bd9989"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Memory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 3: Retrieval**"
      ],
      "metadata": {
        "id": "9_Iyql0i4WLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Embed\n",
        "gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "                     documents=splits,                 # Data\n",
        "                     embedding=gemini_embeddings,    # Embedding model\n",
        "                     #persist_directory=\"./chroma_db\" # Directory to save data\n",
        "                     )\n",
        "\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})"
      ],
      "metadata": {
        "id": "Z4qhBpUstp3A"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = retriever.invoke(\"What is Task Decomposition?\")"
      ],
      "metadata": {
        "id": "quPRjMYO15lW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "OUKx9sFK2NLU",
        "outputId": "4524a145-9b77-46d9-a808-76d9f93ad513"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXZ_OGks2HSj",
        "outputId": "07603c77-697f-404f-ce7a-73038054521c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### RETRIEVAL and GENERATION ####\n",
        "\n",
        "# Prompt\n",
        "#The prompt template 'hwchase17/rag-prompt' does not exist. Using a different template instead.\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")"
      ],
      "metadata": {
        "id": "LEYtD6Ektp0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10fae259-7b28-4579-ae54-9f0e9451ec03"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langsmith/client.py:323: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt"
      ],
      "metadata": {
        "id": "EKOuDKrDtpxV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74d19fa2-d7b2-492f-d089-4b36619c5da4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 4: generation**"
      ],
      "metadata": {
        "id": "NaIBLy7l4lpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Prompt\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "prompt"
      ],
      "metadata": {
        "id": "lEpDq4nNtps1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2a5dac3-068b-4758-cce5-784ddae11f24"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'), additional_kwargs={})])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",\n",
        "                 temperature=0.7, top_p=0.85)"
      ],
      "metadata": {
        "id": "D7wBf5P9tpqG"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain= prompt|llm"
      ],
      "metadata": {
        "id": "qywyGm0Atpng"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run\n",
        "chain.invoke({\"context\":docs,\"question\":\"What is Task Decomposition?\"})"
      ],
      "metadata": {
        "id": "mGEPIOGCtpk_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d04ba428-8024-48b0-8c64-58d37989ebac"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Task Decomposition is a technique used to break down complex tasks into smaller, simpler steps. This is achieved by using Chain of thought (CoT) prompting, which instructs the model to think step by step. This allows the model to utilize more computation at test time and make the task easier to manage. \\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-17c29f22-ee1d-42e1-9e31-ae026df4f0a6-0', usage_metadata={'input_tokens': 190, 'output_tokens': 60, 'total_tokens': 250})"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "prompt_hub_rag = hub.pull(\"rlm/rag-prompt\")"
      ],
      "metadata": {
        "id": "bdtGNUTytpiZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a7f0277-31b8-49ac-928a-1f89becff24f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langsmith/client.py:323: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_hub_rag"
      ],
      "metadata": {
        "id": "mjNgcvsCtpfr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17c229e6-7ca3-4d39-c742-5360df47fb0d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Rag Chain**"
      ],
      "metadata": {
        "id": "J9Ha2Dy56M6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Question\n",
        "rag_chain.invoke(\"What is Task Decomposition?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "vTlfRi3W6Hrp",
        "outputId": "18513db5-0f01-4ffe-8966-cc3271931718"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Task Decomposition is a process where a complex task is broken down into smaller, simpler steps. This is achieved by using a prompting technique called Chain of Thought (CoT), where the model is instructed to \"think step by step\" to break down the task. \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Langchain Practice**"
      ],
      "metadata": {
        "id": "ywiY4phDWSmi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi3aikQ1WJLc"
      },
      "source": [
        "# ***Custom Document Loader***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdowuUGZPGjg"
      },
      "source": [
        "# **Retrieval-Augmented-Generation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "L6dt-SKtpxxU"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-core"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkEnyQYtWFN_"
      },
      "outputs": [],
      "source": [
        "from typing import AsyncIterator, Iterator\n",
        "\n",
        "from langchain_core.document_loaders import BaseLoader\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "\n",
        "class CustomDocumentLLoader(BaseLoader):\n",
        "    \"\"\"An example document loader that reads a file line by line.\"\"\"\n",
        "\n",
        "    def __init__(self, file_path: str) -> None:\n",
        "        \"\"\"Initialize the loader with a file path.\n",
        "\n",
        "        Args:\n",
        "            file_path: The path to the file to load.\n",
        "        \"\"\"\n",
        "        self.file_path = file_path\n",
        "\n",
        "    def lazy_load(self) -> Iterator[Document]:  # <-- Does not take any arguments\n",
        "        \"\"\"A lazy loader that reads a file line by line.\n",
        "\n",
        "        When you're implementing lazy load methods, you should use a generator\n",
        "        to yield documents one by one.\n",
        "        \"\"\"\n",
        "        with open(self.file_path, encoding=\"utf-8\") as f:\n",
        "            line_number = 0\n",
        "            for line in f:\n",
        "                yield Document(\n",
        "                    page_content=line,\n",
        "                    metadata={\"line_number\": line_number, \"source\": self.file_path},\n",
        "                )\n",
        "                line_number += 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBtXaf3iWFLG"
      },
      "outputs": [],
      "source": [
        "with open(\"./meow.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    quality_content = \"meow meow🐱 \\n meow meow🐱 \\n meow😻😻\"\n",
        "    f.write(quality_content)\n",
        "\n",
        "loader = CustomDocumentLLoader(\"./meow.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqP8WQclWFIG"
      },
      "outputs": [],
      "source": [
        "## Test out the lazy load interface\n",
        "for doc in loader.lazy_load():\n",
        "    print(\"--------------\")\n",
        "    print(type(doc))\n",
        "    print(doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zJu4uY8yWFFK"
      },
      "outputs": [],
      "source": [
        "!pip install aiofiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_emizoCKWFCO"
      },
      "outputs": [],
      "source": [
        "from typing import AsyncIterator, Iterator\n",
        "\n",
        "from langchain_core.document_loaders import BaseLoader\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "\n",
        "class CustomDocumentLoader(BaseLoader):\n",
        "    \"\"\"An example document loader that reads a file line by line.\"\"\"\n",
        "\n",
        "    def __init__(self, file_path: str) -> None:\n",
        "        \"\"\"Initialize the loader with a file path.\n",
        "\n",
        "        Args:\n",
        "            file_path: The path to the file to load.\n",
        "        \"\"\"\n",
        "        self.file_path = file_path\n",
        "\n",
        "\n",
        "    # alazy_load is OPTIONAL.\n",
        "    # If you leave out the implementation, a default implementation which delegates to lazy_load will be used!\n",
        "    async def alazy_load(\n",
        "        self,\n",
        "    ) -> AsyncIterator[Document]:  # <-- Does not take any arguments\n",
        "        \"\"\"An async lazy loader that reads a file line by line.\"\"\"\n",
        "        # Requires aiofiles\n",
        "        # Install with `pip install aiofiles`\n",
        "        # https://github.com/Tinche/aiofiles\n",
        "        import aiofiles\n",
        "\n",
        "        async with aiofiles.open(self.file_path, encoding=\"utf-8\") as f:\n",
        "            line_number = 0\n",
        "            async for line in f:\n",
        "                yield Document(\n",
        "                    page_content=line,\n",
        "                    metadata={\"line_number\": line_number, \"source\": self.file_path},\n",
        "                )\n",
        "                line_number += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubcnCASO03ca"
      },
      "outputs": [],
      "source": [
        "loader1 = CustomDocumentLoader(\"./meow.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HWgX_--5WE-6"
      },
      "outputs": [],
      "source": [
        "## Test out the async implementation\n",
        "async for doc in loader1.alazy_load():\n",
        "    print()\n",
        "    print(type(doc))\n",
        "    print(doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cR8um2UkWEyg"
      },
      "outputs": [],
      "source": [
        "data=loader.load()\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73eB-s-vWEvO"
      },
      "outputs": [],
      "source": [
        "data[0].metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-R5jFfzNWErx"
      },
      "outputs": [],
      "source": [
        "data[2].page_content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLaH9l9gwlfC"
      },
      "source": [
        "# **Working with Files**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3pi6hEjWArt"
      },
      "outputs": [],
      "source": [
        "from langchain_core.document_loaders import BaseBlobParser, Blob\n",
        "\n",
        "\n",
        "class MyParser(BaseBlobParser):\n",
        "    \"\"\"A simple parser that creates a document from each line.\"\"\"\n",
        "\n",
        "    def lazy_parse(self, blob: Blob) -> Iterator[Document]:\n",
        "        \"\"\"Parse a blob into a document line by line.\"\"\"\n",
        "        line_number = 0\n",
        "        with blob.as_bytes_io() as f:\n",
        "            for line in f:\n",
        "                line_number += 1\n",
        "                yield Document(\n",
        "                    page_content=line,\n",
        "                    metadata={\"line_number\": line_number, \"source\": blob.source},\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4Qbnj1ZWAoV"
      },
      "outputs": [],
      "source": [
        "blob = Blob.from_path(\"./meow.txt\")\n",
        "parser = MyParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNiicbLu2UY3"
      },
      "outputs": [],
      "source": [
        "parser.lazy_parse(blob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BimYryOWAi3"
      },
      "outputs": [],
      "source": [
        "list(parser.lazy_parse(blob))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EP4lRccaWAf-"
      },
      "outputs": [],
      "source": [
        "blob = Blob(data=b\"some data from memory\\n meow\")\n",
        "list(parser.lazy_parse(blob))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-kwPlst2p9l"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "# Define the data\n",
        "data = [\n",
        "    [\"Name\", \"Age\", \"Occupation\"],\n",
        "    [\"Alice\", 30, \"Engineer\"],\n",
        "    [\"Bob\", 25, \"Designer\"],\n",
        "    [\"Charlie\", 35, \"Teacher\"]\n",
        "]\n",
        "\n",
        "# Specify the file name\n",
        "filename = \"sample_data.csv\"\n",
        "\n",
        "# Write data to CSV file\n",
        "with open(filename, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerows(data)\n",
        "\n",
        "print(f\"CSV file '{filename}' created successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_zKkM_pNpFm"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "# Define the data\n",
        "data = [\n",
        "    [\"Name\", \"Age\", \"Occupation\"],\n",
        "    [\"Alice\", 30, \"Engineer\"],\n",
        "    [\"Bob\", 25, \"Designer\"],\n",
        "    [\"Charlie\", 35, \"Teacher\"]\n",
        "]\n",
        "\n",
        "# Specify the file name\n",
        "filename = \"sample_data2.csv\"\n",
        "\n",
        "# Write data to CSV file\n",
        "with open(filename, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerows(data)\n",
        "\n",
        "print(f\"CSV file '{filename}' created successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ZHZND9NA8pfG"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgGfnfbyWAdB"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "\n",
        "\n",
        "loader = CSVLoader(file_path='/content/sample_data.csv')\n",
        "csv_data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZs0QvMDWAaN"
      },
      "outputs": [],
      "source": [
        "csv_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWsHKxSOWAXN"
      },
      "outputs": [],
      "source": [
        "csv_loader = CSVLoader(file_path='/content/sample_data.csv', csv_args={\n",
        "    'delimiter': ';',\n",
        "    'fieldnames':['Name', 'Age', 'Occuption']\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hrA-VL8_QJV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJPTia239j9C"
      },
      "outputs": [],
      "source": [
        "custom_loader = csv_loader.load()\n",
        "custom_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqTavaoiWAUS"
      },
      "outputs": [],
      "source": [
        "csv_loader_2 = CSVLoader(\n",
        "    file_path='/content/sample_data.csv',\n",
        "    csv_args={\n",
        "        'delimiter': ';',\n",
        "        'fieldnames': ['myName', 'myAge', 'myOccupation']\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m934dwNJWARK"
      },
      "outputs": [],
      "source": [
        "custom_loader = csv_loader_2.load()\n",
        "custom_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUa0w5zBWAOU"
      },
      "outputs": [],
      "source": [
        "csv_loaderr = CSVLoader(\n",
        "    file_path='/content/sample_data.csv',\n",
        "    csv_args={\n",
        "        'delimiter': ';',\n",
        "        'quotechar': '\"',\n",
        "        'escapechar': '\\\\',\n",
        "        'fieldnames': ['Name', 'Age', 'Occupation'],\n",
        "        'skipinitialspace': True,\n",
        "        'quoting': csv.QUOTE_MINIMAL\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a339hFOHWALW"
      },
      "outputs": [],
      "source": [
        "loader = csv_loaderr.load()\n",
        "loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAlo5usMWAE9"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "class CustomCSVLoader:\n",
        "  def __init__(self, file_path, csv_args=None):\n",
        "    self.file_path = file_path\n",
        "    self.csv_args = csv_args or {}\n",
        "\n",
        "  def load(self):\n",
        "    documents = []\n",
        "    # Open and read the CSV file\n",
        "    with open(self.file_path, mode='r', newline='') as file:\n",
        "      reader = csv.DictReader(file, **self.csv_args)\n",
        "      for row_index, row in enumerate(reader):\n",
        "        # Create a Document object for each row\n",
        "        page_content = ', '.join(f'{key}: {value}' for key, value in row.items())\n",
        "        documents.append(Document(\n",
        "                    metadata={'source': self.file_path, 'row': row_index},\n",
        "                    page_content=page_content\n",
        "                ))\n",
        "\n",
        "    return documents\n",
        "\n",
        "# Example usage\n",
        "csv_loader = CustomCSVLoader(\n",
        "    file_path='/content/sample_data.csv',\n",
        "    csv_args={\n",
        "        'delimiter': ',',\n",
        "        'quotechar': '\"'\n",
        "    }\n",
        ")\n",
        "\n",
        "documents = csv_loader.load()\n",
        "for doc in documents:\n",
        "    print(doc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1flNAWhoD-Tj"
      },
      "source": [
        "# **File Directory**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "01Qk5NphPAX-"
      },
      "outputs": [],
      "source": [
        "!pip install unstructured"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6T6P-iMViWw"
      },
      "outputs": [],
      "source": [
        "with open(\"./meow12.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    quality_content = \"meow meow🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱 \\n meow meow🐱 \\n meow😻😻\"\n",
        "    f.write(quality_content)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBIFeWavO1Fg"
      },
      "outputs": [],
      "source": [
        "# prompt: from langchain_community.document_loaders import DirectoryLoader\n",
        "# loader = DirectoryLoader('/content/', glob=\"**/*.csv\", show_progress=True)\n",
        "# data_csv = loader.load()   correvt this code\n",
        "\n",
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "loader = DirectoryLoader('./', glob=\"**/*.txt\", show_progress=True, use_multithreading=True, loader_cls=TextLoader)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6XzNV_kRFGP"
      },
      "outputs": [],
      "source": [
        "docs = loader.load()\n",
        "docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KuWdbxgV__7"
      },
      "outputs": [],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZdbyAmuV_9J"
      },
      "outputs": [],
      "source": [
        "docs[0].page_content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NU3jVV1hWuhi"
      },
      "source": [
        "# **JSON**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TA3jG9nQV_6h"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Define a dictionary with diverse data\n",
        "data = {\n",
        "    \"users\": [\n",
        "        {\n",
        "            \"id\": 1,\n",
        "            \"name\": \"Alice\",\n",
        "            \"age\": 30,\n",
        "            \"email\": \"alice@example.com\",\n",
        "            \"is_active\": True,\n",
        "            \"roles\": [\"admin\", \"editor\"],\n",
        "            \"address\": {\n",
        "                \"city\": \"New York\",\n",
        "                \"zipcode\": \"10001\"\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"id\": 2,\n",
        "            \"name\": \"Bob\",\n",
        "            \"age\": 25,\n",
        "            \"email\": \"bob@example.com\",\n",
        "            \"is_active\": False,\n",
        "            \"roles\": [\"viewer\"],\n",
        "            \"address\": {\n",
        "                \"city\": \"Los Angeles\",\n",
        "                \"zipcode\": \"90001\"\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"id\": 3,\n",
        "            \"name\": \"Charlie\",\n",
        "            \"age\": 35,\n",
        "            \"email\": \"charlie@example.com\",\n",
        "            \"is_active\": True,\n",
        "            \"roles\": [\"admin\", \"viewer\"],\n",
        "            \"address\": {\n",
        "                \"city\": \"Chicago\",\n",
        "                \"zipcode\": \"60601\"\n",
        "            }\n",
        "        }\n",
        "    ],\n",
        "    \"products\": [\n",
        "        {\n",
        "            \"id\": \"p001\",\n",
        "            \"name\": \"Laptop\",\n",
        "            \"price\": 1200.99,\n",
        "            \"quantity\": 10,\n",
        "            \"categories\": [\"electronics\", \"computers\"]\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"p002\",\n",
        "            \"name\": \"Smartphone\",\n",
        "            \"price\": 799.49,\n",
        "            \"quantity\": 50,\n",
        "            \"categories\": [\"electronics\", \"mobile phones\"]\n",
        "        }\n",
        "    ],\n",
        "    \"orders\": [\n",
        "        {\n",
        "            \"order_id\": 101,\n",
        "            \"user_id\": 1,\n",
        "            \"products\": [\n",
        "                {\"id\": \"p001\", \"quantity\": 1},\n",
        "                {\"id\": \"p002\", \"quantity\": 2}\n",
        "            ],\n",
        "            \"total_price\": 2799.97,\n",
        "            \"status\": \"shipped\"\n",
        "        },\n",
        "        {\n",
        "            \"order_id\": 102,\n",
        "            \"user_id\": 2,\n",
        "            \"products\": [\n",
        "                {\"id\": \"p002\", \"quantity\": 1}\n",
        "            ],\n",
        "            \"total_price\": 799.49,\n",
        "            \"status\": \"processing\"\n",
        "        }\n",
        "    ],\n",
        "    \"settings\": {\n",
        "        \"theme\": \"dark\",\n",
        "        \"notifications\": {\n",
        "            \"email\": True,\n",
        "            \"sms\": False\n",
        "        },\n",
        "        \"language\": \"en\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Specify the file name\n",
        "filename = \"diverse_data.json\"\n",
        "\n",
        "# Write data to JSON file\n",
        "with open(filename, 'w') as json_file:\n",
        "    json.dump(data, json_file, indent=4)\n",
        "\n",
        "print(f\"JSON file '{filename}' created successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGrfUjmTV_4C"
      },
      "outputs": [],
      "source": [
        "!pip install jq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCcJiMoqV_1R"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import JSONLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xa3DEctBV_yd"
      },
      "outputs": [],
      "source": [
        "loader = JSONLoader(\n",
        "    file_path='/content/diverse_data.json',\n",
        "    jq_schema='.users[].address',\n",
        "    text_content=False)\n",
        "\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUsAIrn5V_va"
      },
      "outputs": [],
      "source": [
        "data[1].page_content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v5LkAzWaW5D"
      },
      "source": [
        "# **Markdown**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "idAqQGkFa4rZ"
      },
      "outputs": [],
      "source": [
        "!pip install unstructured > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCX2MaLTdn4C"
      },
      "outputs": [],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyUKxjxx5sRO"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xu9EYjmyc8VQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kWsT7AgV_se"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain_community.document_loaders import UnstructuredMarkdownLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4o3BlIoV_p7"
      },
      "outputs": [],
      "source": [
        "markdown_path = \"/content/README.md\"\n",
        "loader = UnstructuredMarkdownLoader('/content/README.md')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Nt7m_ViV_mz"
      },
      "outputs": [],
      "source": [
        "mkr = loader.load()\n",
        "mkr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ds3SkKlvV_kE"
      },
      "outputs": [],
      "source": [
        "mkr[0].metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOvZhAHxV_hM"
      },
      "outputs": [],
      "source": [
        "mkr[0].page_content[0:200]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-76GESQRV_eO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CapbVIutBc92"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fPlGE6m0TKhG"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6jyedNV-VBZW"
      },
      "outputs": [],
      "source": [
        "!pip install rapidocr-onnxruntime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzq0Q8HeO21H"
      },
      "source": [
        "# **PDF LOADER**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08FRMEI7wYPi"
      },
      "outputs": [],
      "source": [
        "%pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SW9LaJX5PAKD"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "loader = PyPDFLoader(\"/content/Generative AI Foundations in Python Discover key techniques and navigate modern challenges in LLMs (Carlos Rodriguez) (Z-Library).pdf\", extract_images=True)\n",
        "pages = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PUcxkDPP7F-"
      },
      "outputs": [],
      "source": [
        "len(pages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9t9R1nz_QCcJ"
      },
      "outputs": [],
      "source": [
        "page = pages[1]\n",
        "page"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbBV2zpFQVvA"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "page.page_content[0:150]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyP2SHlFRCVA"
      },
      "outputs": [],
      "source": [
        "page.metadata[\"source\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrFnhiO-Tu1Q"
      },
      "source": [
        "# **TEXT FILE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vjo6NHnU2Rs"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "loader = TextLoader(\"/content/rag.txt\")\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNik2AaMVwEM"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "data[0].metadata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpTcBruvWqOo"
      },
      "outputs": [],
      "source": [
        "\n",
        "data[0].metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ps2tRKyCXyJI"
      },
      "source": [
        "# **DOCS LOADER**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQ5qjxsEYDWt"
      },
      "outputs": [],
      "source": [
        "!pip install docx2txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntyfGHTEYJGF"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import Docx2txtLoader\n",
        "\n",
        "loader = Docx2txtLoader(\"/content/rag.docx\")\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-YRey6qYVFI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAAH7u2j97W0"
      },
      "source": [
        "# ***TEXT SPLITTERS***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWGD4OHHIhYe"
      },
      "outputs": [],
      "source": [
        "%pip install -qU langchain-text-splitters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7MOhyz597S-"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01BnEMMoOvhT"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
        "\n",
        "html_string = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<body>\n",
        "    <div>\n",
        "        <h1>Foo</h1>\n",
        "        <p>Some intro text about Foo.</p>\n",
        "        <div>\n",
        "            <h2>Bar main section</h2>\n",
        "            <p>Some intro text about Bar.</p>\n",
        "            <h3>Bar subsection 1</h3>\n",
        "            <p>Some text about the first subtopic of Bar.</p>\n",
        "            <h3>Bar subsection 2</h3>\n",
        "            <p>Some text about the second subtopic of Bar.</p>\n",
        "        </div>\n",
        "        <div>\n",
        "            <h2>Baz</h2>\n",
        "            <p>Some text about Baz</p>\n",
        "        </div>\n",
        "        <br>\n",
        "        <p>Some concluding text about Foo</p>\n",
        "    </div>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "headers_to_split_on = [\n",
        "    (\"h1\", \"Header 1\"),\n",
        "    (\"h2\", \"Header 2\"),\n",
        "    (\"h3\", \"Header 3\"),\n",
        "]\n",
        "\n",
        "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
        "html_header_splits = html_splitter.split_text(html_string)\n",
        "html_header_splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glkUhAH1WB8n"
      },
      "outputs": [],
      "source": [
        "%pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhayeJP6U3Du"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
        "\n",
        "\n",
        "url = \"https://plato.stanford.edu/entries/goedel/\"\n",
        "\n",
        "headers_to_split_on = [\n",
        "    (\"h1\", \"Header 1\"),\n",
        "    (\"h2\", \"Header 2\"),\n",
        "    (\"h3\", \"Header 3\"),\n",
        "    (\"h4\", \"Header 4\"),\n",
        "]\n",
        "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
        "html_url_splits= html_splitter.split_text_from_url(url)\n",
        "\n",
        "chunk_size = 500\n",
        "chunk_overlap = 30\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=chunk_size,\n",
        "    chunk_overlap=chunk_overlap\n",
        ")\n",
        "\n",
        "html_header_splits = text_splitter.split_documents(html_url_splits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbW9SpnBV05G"
      },
      "outputs": [],
      "source": [
        "html_header_splits[80:85]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gycsxYdNWfkA"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "url = \"https://plato.stanford.edu/entries/goedel/\"\n",
        "\n",
        "headers_to_split_on = [\n",
        "    (\"h1\", \"Header 1\"),\n",
        "    (\"h2\", \"Header 2\"),\n",
        "    (\"h3\", \"Header 3\"),\n",
        "    (\"h4\", \"Header 4\"),\n",
        "]\n",
        "\n",
        "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
        "\n",
        "# for local file use html_splitter.split_text_from_file(<path_to_file>)\n",
        "html_header_splits = html_splitter.split_text_from_url(url)\n",
        "\n",
        "chunk_size = 500\n",
        "chunk_overlap = 30\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
        ")\n",
        "\n",
        "# Split\n",
        "splits = text_splitter.split_documents(html_header_splits)\n",
        "splits[81].page_content\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2a41usLXAeb"
      },
      "outputs": [],
      "source": [
        "len(splits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAkrP8vmdt2w"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import HTMLSectionSplitter\n",
        "\n",
        "html_string = \"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html>\n",
        "    <body>\n",
        "        <div>\n",
        "            <h1>Foo</h1>\n",
        "            <p>Some intro text about Foo.</p>\n",
        "            <div>\n",
        "                <h2>Bar main section</h2>\n",
        "                <p>Some intro text about Bar.</p>\n",
        "                <h3>Bar subsection 1</h3>\n",
        "                <p>Some text about the first subtopic of Bar.</p>\n",
        "                <h3>Bar subsection 2</h3>\n",
        "                <p>Some text about the second subtopic of Bar.</p>\n",
        "            </div>\n",
        "            <div>\n",
        "                <h2>Baz</h2>\n",
        "                <p>Some text about Baz</p>\n",
        "            </div>\n",
        "            <br>\n",
        "            <p>Some concluding text about Foo</p>\n",
        "        </div>\n",
        "    </body>\n",
        "    </html>\n",
        "\"\"\"\n",
        "\n",
        "headers_to_split_on = [(\"h1\", \"Header 1\"), (\"h3\", \"Header 3\")]\n",
        "\n",
        "html_splitter = HTMLSectionSplitter(headers_to_split_on=headers_to_split_on)\n",
        "html_header_splits = html_splitter.split_text(html_string)\n",
        "html_header_splits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TX3fCDC8yyP3"
      },
      "source": [
        "# **Split by character**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVnEG1-Ewyid"
      },
      "outputs": [],
      "source": [
        "%pip install -qU langchain-text-splitters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNwZd-Cny07b"
      },
      "outputs": [],
      "source": [
        "# This is a long document we can split up.\n",
        "with open(\"/content/math.txt\") as f:\n",
        "    state_of_the_union = f.read()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okUw81pmzDf3"
      },
      "outputs": [],
      "source": [
        "print(state_of_the_union[0:250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtTf8xtJzYnh"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    #separator=\"\\n\\n\",\n",
        "    chunk_size=100,\n",
        "    chunk_overlap=10,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jf_uKr5LzsEQ"
      },
      "outputs": [],
      "source": [
        "texts = text_splitter.create_documents([state_of_the_union])\n",
        "print(texts[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXy0eUabzz5G"
      },
      "outputs": [],
      "source": [
        "%pip install -qU langchain-text-splitters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import (\n",
        "    Language,\n",
        "    RecursiveCharacterTextSplitter,\n",
        ")"
      ],
      "metadata": {
        "id": "znZ2YDo9Tmxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Language"
      ],
      "metadata": {
        "id": "EAUkp1dST53r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Full list of supported languages\n",
        "[e.value for e in Language]"
      ],
      "metadata": {
        "id": "1L0vqopsTqrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can also see the separators used for a given language\n",
        "RecursiveCharacterTextSplitter.get_separators_for_language(Language.RUST)"
      ],
      "metadata": {
        "id": "iEpKCem2UsXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PYTHON_CODE = \"\"\"\n",
        "def hello_world():\n",
        "    print(\"Hello, World!\")\n",
        "\n",
        "# Call the function\n",
        "hello_world()\n",
        "\"\"\"\n",
        "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "    language=Language.PYTHON, chunk_size=15, chunk_overlap=0\n",
        ")\n",
        "python_docs = python_splitter.create_documents([PYTHON_CODE])\n",
        "python_docs"
      ],
      "metadata": {
        "id": "zqC82-MqVH7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "python_docs[1]"
      ],
      "metadata": {
        "id": "mL32Pd8sVs62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "import requests"
      ],
      "metadata": {
        "id": "DOCeNZAVYAPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveJsonSplitter"
      ],
      "metadata": {
        "id": "4Gs8J09tYEDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a large nested json object and will be loaded as a python dict\n",
        "json_data = requests.get(\"https://api.smith.langchain.com/openapi.json\").json()\n",
        "json_data[\"info\"].items()"
      ],
      "metadata": {
        "id": "RlX-SRnHYKYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = RecursiveJsonSplitter(max_chunk_size=300)\n",
        "# Recursively split json data - If you need to access/manipulate the smaller json chunks\n",
        "json_chunks = splitter.split_json(json_data=json_data)"
      ],
      "metadata": {
        "id": "PCdV95_hd1XD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_chunks[0]"
      ],
      "metadata": {
        "id": "Pd8BpfrbehxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The splitter can also output documents\n",
        "docs = splitter.create_documents(texts=[json_data])\n",
        "\n",
        "# or a list of strings\n",
        "texts = splitter.split_text(json_data=json_data)\n",
        "\n",
        "print(texts[0])\n",
        "print(texts[1])"
      ],
      "metadata": {
        "id": "nQTBxhWeeknG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts)"
      ],
      "metadata": {
        "id": "oC6aam_ef267"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0]"
      ],
      "metadata": {
        "id": "WPuR27aLfJ7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's look at the size of the chunks\n",
        "print([len(text) for text in texts][:100])\n",
        "\n",
        "# Reviewing one of these chunks that was bigger we see there is a list object there\n",
        "print(len(texts[1]))"
      ],
      "metadata": {
        "id": "7Z0xO_FgfNMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet langchain-text-splitters tiktoken"
      ],
      "metadata": {
        "id": "ny1i5la2fopZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a long document we can split up.\n",
        "with open(\"/content/meow12.txt\") as f:\n",
        "    state_of_the_union = f.read()\n",
        "from langchain_text_splitters import CharacterTextSplitter"
      ],
      "metadata": {
        "id": "CewRuIG4KW3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
        "   chunk_size=100, chunk_overlap=0, encoding=\"cl100k_base\"\n",
        ")\n",
        "texts = text_splitter.split_text(state_of_the_union)"
      ],
      "metadata": {
        "id": "NfWj04cAKtCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    model_name=\"gpt-4\",\n",
        "    chunk_size=100,\n",
        "    chunk_overlap=0,\n",
        ")\n",
        "texts = text_splitter.split_text(state_of_the_union)"
      ],
      "metadata": {
        "id": "ZHF-tiOJLpvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts[0]"
      ],
      "metadata": {
        "id": "annw1ZeuMCTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import TokenTextSplitter\n",
        "\n",
        "text_splitter = TokenTextSplitter(chunk_size=10, chunk_overlap=0)\n",
        "\n",
        "texts = text_splitter.split_text(state_of_the_union)\n",
        "print(len(texts[2]))"
      ],
      "metadata": {
        "id": "ownP4JfkL4zA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet  spacy"
      ],
      "metadata": {
        "id": "f5zWb26SQQ-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a long document we can split up.\n",
        "with open(\"/content/meow12.txt\") as f:\n",
        "    state_of_the_union = f.read()\n",
        "from langchain_text_splitters import SpacyTextSplitter"
      ],
      "metadata": {
        "id": "g4vyT1H3QZGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import SpacyTextSplitter\n",
        "\n",
        "text_splitter = SpacyTextSplitter(chunk_size=1000)"
      ],
      "metadata": {
        "id": "E1uUMcg3Qi6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = text_splitter.split_text(state_of_the_union)\n",
        "texts"
      ],
      "metadata": {
        "id": "HA099fWIQwpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "ToGVPjOYRlyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import SentenceTransformersTokenTextSplitter"
      ],
      "metadata": {
        "id": "_7or9ZVYRWCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0)\n",
        "text = \"Lorem \""
      ],
      "metadata": {
        "id": "Z2itKmxgRaUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_start_and_stop_tokens = 2\n",
        "text_token_count = splitter.count_tokens(text=text) - count_start_and_stop_tokens\n",
        "print(text_token_count)"
      ],
      "metadata": {
        "id": "uxZwAzlzSJhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_multiplier = splitter.maximum_tokens_per_chunk // text_token_count + 1"
      ],
      "metadata": {
        "id": "-fTI3k72St7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_multiplier"
      ],
      "metadata": {
        "id": "M2WG34lvSvZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_to_split = text * token_multiplier\n",
        "text_to_split"
      ],
      "metadata": {
        "id": "MGK_NJlpT7_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_multiplier = splitter.maximum_tokens_per_chunk // text_token_count + 1\n",
        "\n",
        "# `text_to_split` does not fit in a single chunk\n",
        "text_to_split = text * token_multiplier\n",
        "\n",
        "print(f\"tokens in text to split: {splitter.count_tokens(text=text_to_split)}\")"
      ],
      "metadata": {
        "id": "f6E1gC6TTGe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_chunks = splitter.split_text(text=text_to_split)\n",
        "\n",
        "print(text_chunks[1])"
      ],
      "metadata": {
        "id": "UtvHEdsqTYRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet langchain_experimental langchain_openai"
      ],
      "metadata": {
        "id": "IKyllcPmfXh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a long document we can split up.\n",
        "with open(\"/content/meow12.txt\") as f:\n",
        "    state_of_the_union = f.read()"
      ],
      "metadata": {
        "id": "tTKyMsz2fgN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings"
      ],
      "metadata": {
        "id": "EDVramKtflSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = SemanticChunker(OpenAIEmbeddings())"
      ],
      "metadata": {
        "id": "D9NWHr1MflPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1owLRtQgflMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PIPELINE **"
      ],
      "metadata": {
        "id": "w0b5T1EL41Am"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_text_splitters"
      ],
      "metadata": {
        "id": "dBgjz0ztfkqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UyG7feS96Ve3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "id": "L3cqPAvR6qBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "id": "ctq-L-lW7OhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter"
      ],
      "metadata": {
        "id": "pSEd-pdD5QbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "loader = PyPDFLoader(\"/content/Prompt Engineering for Generative AI Future-Proof Inputs for Reliable Al Outputs (James Phoenix, Mike Taylor).pdf\")\n",
        "pages = loader.load()"
      ],
      "metadata": {
        "id": "84bTBhz66kap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_size = 26\n",
        "chunk_overlap = 4\n",
        "\n",
        "r_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
        ")\n",
        "\n",
        "c_splitter = CharacterTextSplitter(\n",
        "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
        ")\n"
      ],
      "metadata": {
        "id": "9eux49145giy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_list = []\n",
        "for page in pages:\n",
        "  chunks = r_splitter.split_text(page.page_content)\n",
        "  for chunk in chunks:\n",
        "    chunk_list.append(chunk)\n",
        "\n"
      ],
      "metadata": {
        "id": "wuHQxhhD51Hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(chunk_list))"
      ],
      "metadata": {
        "id": "jUr4hbfE8jZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_list[6]"
      ],
      "metadata": {
        "id": "GMHl-lbs82yG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers langchain_huggingface"
      ],
      "metadata": {
        "id": "TyzQl9V-82vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "emb_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")"
      ],
      "metadata": {
        "id": "jRJR10DF82tL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generate embedding\n",
        "embeddings = emb_model.embed_query(\"how\")\n",
        "len(embeddings)"
      ],
      "metadata": {
        "id": "j3vxp--G82qV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = emb_model.embed_query(chunk_list[5])\n",
        "len(embeddings)"
      ],
      "metadata": {
        "id": "yls4Ih-d82kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings"
      ],
      "metadata": {
        "id": "UZ-XStnTDlOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **VECTORE STORE**"
      ],
      "metadata": {
        "id": "rc4XIFUGRn5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community langchain_huggingface qdrant_client langchain_qdrant pypdf openai langchain_core langchain"
      ],
      "metadata": {
        "id": "yBh8BNz3SD59",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from qdrant_client import QdrantClient\n",
        "from langchain_core.documents import Document\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_qdrant import QdrantVectorStore\n",
        "import openai\n",
        "import os"
      ],
      "metadata": {
        "id": "52KNivIuSD3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Embedding Model**"
      ],
      "metadata": {
        "id": "-7hHUiYtWJ3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize embedding model\n",
        "\n",
        "emb_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vuKIYHtcSD0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading the data**"
      ],
      "metadata": {
        "id": "Z7kjLtH4W6qI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the Pdf documents using PyPdfLoader\n",
        "loaders = PyPDFLoader(\"/content/Prompt Engineering for Generative AI Future-Proof Inputs for Reliable Al Outputs (James Phoenix, Mike Taylor).pdf\")\n",
        "pages = loaders.load()"
      ],
      "metadata": {
        "id": "lqVRxjqdV1Zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(pages)"
      ],
      "metadata": {
        "id": "B-4fVZnbSDyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pages[0:20]"
      ],
      "metadata": {
        "id": "ypVQwzanSDvv",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Splitting the document into chunks**"
      ],
      "metadata": {
        "id": "h4Qu_KTaZcDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1500,\n",
        "    chunk_overlap=150,\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "NaAMGq5RZbog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.docstore.document import Document"
      ],
      "metadata": {
        "id": "eyRGNrVfSDgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create an empty list to store the processed documents\n",
        "docs_list = []\n",
        "#Iterate over each page in the extracted pages\n",
        "for page in pages[0:20]:\n",
        "  #split the page content into smaller chunks\n",
        "  page_split = text_splitter.split_text(page.page_content)\n",
        "\n",
        "  # Iterate over the each chunk and craete Document object\n",
        "  for page_sub_split in page_split:\n",
        "    #Metadata for each chunk, including source and page number\n",
        "    metadata = {\"source\":\"Prompt Engineering\", \"page_no\":page.metadata[\"page\"]+1}\n",
        "    #Create a Document object with the chunk content and metadata\n",
        "    doc_string = Document(page_content=page_sub_split, metadata=metadata)\n",
        "    #Append the doc_string to the list\n",
        "    docs_list.append(doc_string)"
      ],
      "metadata": {
        "id": "FsDpnzvPcIIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_list[0]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RuHfHrLRcIFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs_list)"
      ],
      "metadata": {
        "id": "-iMEEMVdcICJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PrQ2vKbfhrsf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Qdrant Vector store**"
      ],
      "metadata": {
        "id": "a0yZqejMgoDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Qdrant Credentials\n",
        "#qdrant_url = \"http://localhost:6333\"\n",
        "#qdrant_api_key = \"_ggggggggg\"\n",
        "#collection_name = \"prompt_engineering\""
      ],
      "metadata": {
        "id": "s-5fHDbLcHuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Qdrantvector store with documents and embedding model\n",
        "# qdrant = QdrantVectorStore.from_documents(\n",
        "#     documents=docs_list,\n",
        "#     embedding=emb_model,\n",
        "#     url=qdrant_url,\n",
        "#     api_key=qdrant_api_key,\n",
        "#     collection_name=collection_name)"
      ],
      "metadata": {
        "id": "3799gk9JcHsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Query vector store\n",
        "# query = \"what is the purpose of prompt engineering\"\n",
        "# result = qdrant.similarity_search(query, k= 5)\n",
        "# result[3]"
      ],
      "metadata": {
        "id": "P6VcWOSIcHoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pinecone vector store**"
      ],
      "metadata": {
        "id": "FZu2cG-kj2j5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain-pinecone pinecone-notebooks"
      ],
      "metadata": {
        "id": "ZIV6TnMccHli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PINECONE_API_KEY=\"2cb5589e-2b4f-4c1d-a8e1-afdb47d820d3\"\n",
        "index_name=\"prompt-engineering\""
      ],
      "metadata": {
        "id": "c2vi8GhucHjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data upsertion in pinecone\n",
        "from langchain_pinecone import PineconeVectorStore as langchain_pinecone\n",
        "import os\n",
        "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n"
      ],
      "metadata": {
        "id": "vtxrVg3BcHgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector = langchain_pinecone.from_documents(\n",
        "    docs_list,\n",
        "    emb_model,\n",
        "    index_name=index_name\n",
        ")"
      ],
      "metadata": {
        "id": "pfj2EiTxcHdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is midjourney\"\n",
        "pinecone_result = vector.similarity_search(query, k= 1)\n"
      ],
      "metadata": {
        "id": "g7u7bWVGcHan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pinecone_result[0].page_content"
      ],
      "metadata": {
        "id": "zKg_9hbe2ec_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pinecone"
      ],
      "metadata": {
        "id": "O0J4eZTfcHYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "# pc = Pinecone(api_key=\"43aa6948-ee7e-4fca-bf6f-3426a0e20f3b\")"
      ],
      "metadata": {
        "id": "0CEUNllKcHVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# index_name=\"prompt-engineering\"\n",
        "\n",
        "# pc.create_index(\n",
        "#     name=index_name,\n",
        "#     dimension=384, # Replace with your model dimensions\n",
        "#     metric=\"cosine\", # Replace with your model metric\n",
        "#     spec=ServerlessSpec(\n",
        "#         cloud=\"aws\",\n",
        "#         region=\"us-east-1\"\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "kKe-4VOTcHSr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPsAM9ee6ix7jNY82GTfPvu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}